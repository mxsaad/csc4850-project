{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c048c96-c80d-4230-80cb-ab5ca055cf96",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd337e62-5c8a-427b-bc4c-706e51672942",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16bf67a-fcaa-481f-bf6c-67f43c0bf110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 1 - Dimensions: (150, 3312)\n",
      "Train Label 1 - Dimensions: (150, 1)\n",
      "Test Data 1 - Dimensions: (53, 3312)\n",
      "Train Data 1 - Missing Values: 9936\n",
      "Test Data 1 - Missing Values: 7021\n",
      "----------------------------------------\n",
      "Train Data 2 - Dimensions: (100, 9182)\n",
      "Train Label 2 - Dimensions: (100, 1)\n",
      "Test Data 2 - Dimensions: (74, 9182)\n",
      "Train Data 2 - Missing Values: 0\n",
      "Test Data 2 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 3 - Dimensions: (6300, 13)\n",
      "Train Label 3 - Dimensions: (6300, 1)\n",
      "Test Data 3 - Dimensions: (2693, 13)\n",
      "Train Data 3 - Missing Values: 1886\n",
      "Test Data 3 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 4 - Dimensions: (2547, 112)\n",
      "Train Label 4 - Dimensions: (2547, 1)\n",
      "Test Data 4 - Dimensions: (1092, 112)\n",
      "Train Data 4 - Missing Values: 0\n",
      "Test Data 4 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 5 - Dimensions: (1119, 11)\n",
      "Train Label 5 - Dimensions: (1119, 1)\n",
      "Test Data 5 - Dimensions: (480, 11)\n",
      "Train Data 5 - Missing Values: 0\n",
      "Test Data 5 - Missing Values: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# File names for training and test datasets\n",
    "train_files = [\"TrainData1.txt\", \"TrainData2.txt\", \"TrainData3.txt\", \"TrainData4.txt\", \"TrainData5.txt\"]\n",
    "train_label_files = [\"TrainLabel1.txt\", \"TrainLabel2.txt\", \"TrainLabel3.txt\", \"TrainLabel4.txt\", \"TrainLabel5.txt\"]\n",
    "test_files = [\"TestData1.txt\", \"TestData2.txt\", \"TestData3.txt\", \"TestData4.txt\", \"TestData5.txt\"]\n",
    "\n",
    "# Load each dataset into separate variables\n",
    "train_datasets = {}\n",
    "train_labels = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for i in range(len(train_files)):\n",
    "    # Load training data and label using read_table for whitespace-separated data\n",
    "    train_datasets[f\"train_data_{i+1}\"] = pd.read_table(os.path.join(data_dir, train_files[i]), header=None, delim_whitespace=True)\n",
    "    train_labels[f\"train_label_{i+1}\"] = pd.read_table(os.path.join(data_dir, train_label_files[i]), header=None, delim_whitespace=True)\n",
    "    test_datasets[f\"test_data_{i+1}\"] = pd.read_table(os.path.join(data_dir, test_files[i]), header=None, delim_whitespace=True)\n",
    "    \n",
    "    # Display basic information for each dataset\n",
    "    print(f\"Train Data {i+1} - Dimensions: {train_datasets[f'train_data_{i+1}'].shape}\")\n",
    "    print(f\"Train Label {i+1} - Dimensions: {train_labels[f'train_label_{i+1}'].shape}\")\n",
    "    print(f\"Test Data {i+1} - Dimensions: {test_datasets[f'test_data_{i+1}'].shape}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_count = (train_datasets[f\"train_data_{i+1}\"] == 1.00000000000000e+99).sum().sum()\n",
    "    print(f\"Train Data {i+1} - Missing Values: {missing_count}\")\n",
    "    \n",
    "    missing_count_test = (test_datasets[f\"test_data_{i+1}\"] == 1.00000000000000e+99).sum().sum()\n",
    "    print(f\"Test Data {i+1} - Missing Values: {missing_count_test}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3b558-3593-497e-a30e-85809aef62b7",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd73fce-454f-4998-a2a8-c3fd30d31119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train Data 1 - Dimensions: (150, 3312)\n",
      "Processed Test Data 1 - Dimensions: (53, 3312)\n",
      "----------------------------------------\n",
      "Processed Train Data 2 - Dimensions: (100, 9182)\n",
      "Processed Test Data 2 - Dimensions: (74, 9182)\n",
      "----------------------------------------\n",
      "Processed Train Data 3 - Dimensions: (6300, 13)\n",
      "Processed Test Data 3 - Dimensions: (2693, 13)\n",
      "----------------------------------------\n",
      "Processed Train Data 4 - Dimensions: (2547, 112)\n",
      "Processed Test Data 4 - Dimensions: (1092, 112)\n",
      "----------------------------------------\n",
      "Processed Train Data 5 - Dimensions: (1119, 11)\n",
      "Processed Test Data 5 - Dimensions: (480, 11)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loop through each dataset to preprocess\n",
    "for i in range(len(train_files)):\n",
    "    # Load the datasets again in case they were modified\n",
    "    train_data = train_datasets[f\"train_data_{i+1}\"]\n",
    "    test_data = test_datasets[f\"test_data_{i+1}\"]\n",
    "    \n",
    "    # Step 1: Handle missing values (replace 1.00000000000000e+99 with NaN)\n",
    "    train_data.replace(1.00000000000000e+99, np.nan, inplace=True)\n",
    "    test_data.replace(1.00000000000000e+99, np.nan, inplace=True)\n",
    "    \n",
    "    # For numerical features, we will use the mean to fill missing values\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    test_data.fillna(test_data.mean(), inplace=True)\n",
    "    \n",
    "    # Step 2: Standardize high-dimensional datasets (only for those with many features)\n",
    "    # Standardize TrainData1 and TrainData2 (with many features)\n",
    "    if train_data.shape[1] > 100:  # If the dataset has a large number of features\n",
    "        scaler = StandardScaler()\n",
    "        train_data = scaler.fit_transform(train_data)\n",
    "        test_data = scaler.transform(test_data)\n",
    "    \n",
    "    # Save the processed data back into the datasets dictionary\n",
    "    train_datasets[f\"train_data_{i+1}\"] = train_data\n",
    "    test_datasets[f\"test_data_{i+1}\"] = test_data\n",
    "    \n",
    "    # Step 3: Print the updated info\n",
    "    print(f\"Processed Train Data {i+1} - Dimensions: {train_data.shape}\")\n",
    "    print(f\"Processed Test Data {i+1} - Dimensions: {test_data.shape}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50e561-a574-4d02-964a-e04210d726ba",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "323006d2-d2b2-445d-942f-dd618690a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM for TrainData 1...\n",
      "Model for TrainData 1 saved.\n",
      "----------------------------------------\n",
      "Training SVM for TrainData 2...\n",
      "Model for TrainData 2 saved.\n",
      "----------------------------------------\n",
      "Training KNN for TrainData 3...\n",
      "Model for TrainData 3 saved.\n",
      "----------------------------------------\n",
      "Training SVM for TrainData 4...\n",
      "Model for TrainData 4 saved.\n",
      "----------------------------------------\n",
      "Training KNN for TrainData 5...\n",
      "Model for TrainData 5 saved.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib  # To save the trained models\n",
    "\n",
    "# Function to select and train models\n",
    "def model_selection_and_training(train_data, train_label, test_data, dataset_index):\n",
    "    if train_data.shape[1] > 100:  # Many features, fewer samples: Use SVM\n",
    "        model = SVC(kernel='linear')  # Linear kernel for simplicity\n",
    "        print(f\"Training SVM for TrainData {dataset_index}...\")\n",
    "    else:  # Few features, many samples: Use KNN\n",
    "        model = KNeighborsClassifier(n_neighbors=5)  # Using 5 neighbors for KNN\n",
    "        print(f\"Training KNN for TrainData {dataset_index}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, train_label)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(model, f\"../models/model_{dataset_index}.pkl\")\n",
    "    print(f\"Model for TrainData {dataset_index} saved.\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Loop through each dataset and apply the model selection and training process\n",
    "for i in range(1, 6):\n",
    "    # Load the training data and labels\n",
    "    train_data = train_datasets[f\"train_data_{i}\"]\n",
    "    train_label = train_labels[f\"train_label_{i}\"].values.flatten()  # Flattening the label array\n",
    "    \n",
    "    # Call the function to train the model\n",
    "    model_selection_and_training(train_data, train_label, test_datasets[f\"test_data_{i}\"], i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca1961-c8c8-45c3-b421-27a75c9d192f",
   "metadata": {},
   "source": [
    "## 4. Store Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2de81b-604a-466b-af14-846338bd6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for Test Data 1...\n",
      "Predictions for Test Data 1 saved to ../results/predictions_test_1.txt\n",
      "----------------------------------------\n",
      "Making predictions for Test Data 2...\n",
      "Predictions for Test Data 2 saved to ../results/predictions_test_2.txt\n",
      "----------------------------------------\n",
      "Making predictions for Test Data 3...\n",
      "Predictions for Test Data 3 saved to ../results/predictions_test_3.txt\n",
      "----------------------------------------\n",
      "Making predictions for Test Data 4...\n",
      "Predictions for Test Data 4 saved to ../results/predictions_test_4.txt\n",
      "----------------------------------------\n",
      "Making predictions for Test Data 5...\n",
      "Predictions for Test Data 5 saved to ../results/predictions_test_5.txt\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# File paths\n",
    "model_dir = \"../models\"\n",
    "results_dir = \"../results\"\n",
    "\n",
    "# Loop through each dataset, load model, make predictions and save the results\n",
    "for i in range(1, 6):  # Loop through datasets 1 to 5\n",
    "    print(f\"Making predictions for Test Data {i}...\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load(os.path.join(model_dir, f\"model_{i}.pkl\"))\n",
    "    \n",
    "    # Get the corresponding test data\n",
    "    test_data = test_datasets[f\"test_data_{i}\"]\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    # Save the predictions to a file\n",
    "    results_file = os.path.join(results_dir, f\"predictions_test_{i}.txt\")\n",
    "    np.savetxt(results_file, predictions, fmt='%d', delimiter=\"\\n\")\n",
    "    \n",
    "    print(f\"Predictions for Test Data {i} saved to {results_file}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3232316-0d33-45f2-a688-b4be55c8ec95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
