{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c048c96-c80d-4230-80cb-ab5ca055cf96",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd337e62-5c8a-427b-bc4c-706e51672942",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16bf67a-fcaa-481f-bf6c-67f43c0bf110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data 1 - Dimensions: (120, 3312)\n",
      "Validation Data 1 - Dimensions: (30, 3312)\n",
      "Test Data 1 - Dimensions: (53, 3312)\n",
      "Train Data 1 - Missing Values: 8013\n",
      "Validation Data 1 - Missing Values: 1923\n",
      "Test Data 1 - Missing Values: 7021\n",
      "----------------------------------------\n",
      "Train Data 2 - Dimensions: (80, 9182)\n",
      "Validation Data 2 - Dimensions: (20, 9182)\n",
      "Test Data 2 - Dimensions: (74, 9182)\n",
      "Train Data 2 - Missing Values: 0\n",
      "Validation Data 2 - Missing Values: 0\n",
      "Test Data 2 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 3 - Dimensions: (5040, 13)\n",
      "Validation Data 3 - Dimensions: (1260, 13)\n",
      "Test Data 3 - Dimensions: (2693, 13)\n",
      "Train Data 3 - Missing Values: 1551\n",
      "Validation Data 3 - Missing Values: 335\n",
      "Test Data 3 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 4 - Dimensions: (2037, 112)\n",
      "Validation Data 4 - Dimensions: (510, 112)\n",
      "Test Data 4 - Dimensions: (1092, 112)\n",
      "Train Data 4 - Missing Values: 0\n",
      "Validation Data 4 - Missing Values: 0\n",
      "Test Data 4 - Missing Values: 0\n",
      "----------------------------------------\n",
      "Train Data 5 - Dimensions: (895, 11)\n",
      "Validation Data 5 - Dimensions: (224, 11)\n",
      "Test Data 5 - Dimensions: (480, 11)\n",
      "Train Data 5 - Missing Values: 0\n",
      "Validation Data 5 - Missing Values: 0\n",
      "Test Data 5 - Missing Values: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# File names for training and test datasets\n",
    "train_files = [\"TrainData1.txt\", \"TrainData2.txt\", \"TrainData3.txt\", \"TrainData4.txt\", \"TrainData5.txt\"]\n",
    "train_label_files = [\"TrainLabel1.txt\", \"TrainLabel2.txt\", \"TrainLabel3.txt\", \"TrainLabel4.txt\", \"TrainLabel5.txt\"]\n",
    "test_files = [\"TestData1.txt\", \"TestData2.txt\", \"TestData3.txt\", \"TestData4.txt\", \"TestData5.txt\"]\n",
    "\n",
    "# Load each dataset into separate variables\n",
    "train_datasets = {}\n",
    "validation_datasets = {}\n",
    "train_labels = {}\n",
    "validation_labels = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for i in range(len(train_files)):\n",
    "    # Load training data and label using read_table for whitespace-separated data\n",
    "    full_train_data = pd.read_table(os.path.join(data_dir, train_files[i]), header=None, delim_whitespace=True)\n",
    "    full_train_label = pd.read_table(os.path.join(data_dir, train_label_files[i]), header=None, delim_whitespace=True)\n",
    "    test_data = pd.read_table(os.path.join(data_dir, test_files[i]), header=None, delim_whitespace=True)\n",
    "    \n",
    "    # Split the training data into train and validation sets\n",
    "    train_data, val_data, train_label, val_label = train_test_split(\n",
    "        full_train_data, full_train_label, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Store datasets in dictionaries\n",
    "    train_datasets[f\"train_data_{i+1}\"] = train_data\n",
    "    validation_datasets[f\"val_data_{i+1}\"] = val_data\n",
    "    train_labels[f\"train_label_{i+1}\"] = train_label\n",
    "    validation_labels[f\"val_label_{i+1}\"] = val_label\n",
    "    test_datasets[f\"test_data_{i+1}\"] = test_data\n",
    "    \n",
    "    # Display basic information for each dataset\n",
    "    print(f\"Train Data {i+1} - Dimensions: {train_data.shape}\")\n",
    "    print(f\"Validation Data {i+1} - Dimensions: {val_data.shape}\")\n",
    "    print(f\"Test Data {i+1} - Dimensions: {test_data.shape}\")\n",
    "    \n",
    "    # Check for missing values in the training and validation data\n",
    "    missing_count_train = (train_data == 1.00000000000000e+99).sum().sum()\n",
    "    missing_count_val = (val_data == 1.00000000000000e+99).sum().sum()\n",
    "    missing_count_test = (test_data == 1.00000000000000e+99).sum().sum()\n",
    "    \n",
    "    print(f\"Train Data {i+1} - Missing Values: {missing_count_train}\")\n",
    "    print(f\"Validation Data {i+1} - Missing Values: {missing_count_val}\")\n",
    "    print(f\"Test Data {i+1} - Missing Values: {missing_count_test}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad3b558-3593-497e-a30e-85809aef62b7",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd73fce-454f-4998-a2a8-c3fd30d31119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Train Data 1 - Dimensions: (120, 3312)\n",
      "Processed Validation Data 1 - Dimensions: (30, 3312)\n",
      "----------------------------------------\n",
      "Processed Train Data 2 - Dimensions: (80, 9182)\n",
      "Processed Validation Data 2 - Dimensions: (20, 9182)\n",
      "----------------------------------------\n",
      "Processed Train Data 3 - Dimensions: (5040, 13)\n",
      "Processed Validation Data 3 - Dimensions: (1260, 13)\n",
      "----------------------------------------\n",
      "Processed Train Data 4 - Dimensions: (2037, 112)\n",
      "Processed Validation Data 4 - Dimensions: (510, 112)\n",
      "----------------------------------------\n",
      "Processed Train Data 5 - Dimensions: (895, 11)\n",
      "Processed Validation Data 5 - Dimensions: (224, 11)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocess each dataset's training and validation data\n",
    "for i in range(len(train_files)):\n",
    "    # Access the train and validation sets\n",
    "    train_data = train_datasets[f\"train_data_{i+1}\"]\n",
    "    val_data = validation_datasets[f\"val_data_{i+1}\"]\n",
    "    \n",
    "    # Step 1: Handle missing values (replace 1.00000000000000e+99 with NaN)\n",
    "    train_data.replace(1.00000000000000e+99, np.nan, inplace=True)\n",
    "    val_data.replace(1.00000000000000e+99, np.nan, inplace=True)\n",
    "    \n",
    "    # For numerical features, we use the mean of the training data to fill missing values\n",
    "    train_data.fillna(train_data.mean(), inplace=True)\n",
    "    val_data.fillna(val_data.mean(), inplace=True)  # Use means from val_data\n",
    "    \n",
    "    # Step 2: Standardize datasets with high dimensionality\n",
    "    if train_data.shape[1] > 100:  # If the dataset has many features\n",
    "        scaler = StandardScaler()\n",
    "        train_data = scaler.fit_transform(train_data)\n",
    "        val_data = scaler.transform(val_data)  # Use the same scaler on validation data\n",
    "    \n",
    "    # Save the processed data back into the datasets dictionary\n",
    "    train_datasets[f\"train_data_{i+1}\"] = train_data\n",
    "    validation_datasets[f\"val_data_{i+1}\"] = val_data\n",
    "    \n",
    "    # Step 3: Print the updated info\n",
    "    print(f\"Processed Train Data {i+1} - Dimensions: {train_data.shape}\")\n",
    "    print(f\"Processed Validation Data {i+1} - Dimensions: {val_data.shape}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50e561-a574-4d02-964a-e04210d726ba",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "323006d2-d2b2-445d-942f-dd618690a383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Dataset 1...\n",
      "Model for Dataset 1 saved.\n",
      "----------------------------------------\n",
      "Training model for Dataset 2...\n",
      "Model for Dataset 2 saved.\n",
      "----------------------------------------\n",
      "Training model for Dataset 3...\n",
      "Model for Dataset 3 saved.\n",
      "----------------------------------------\n",
      "Training model for Dataset 4...\n",
      "Model for Dataset 4 saved.\n",
      "----------------------------------------\n",
      "Training model for Dataset 5...\n",
      "Model for Dataset 5 saved.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC  # Support Vector Machine\n",
    "import joblib  # To save the trained models\n",
    "\n",
    "# Define the model mapping for each dataset\n",
    "model_mapping = {\n",
    "    1: KNeighborsClassifier(n_neighbors=5),  # Dataset 1: KNN\n",
    "    2: RandomForestClassifier(n_estimators=100, random_state=42),  # Dataset 2: Random Forest\n",
    "    3: SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42),  # Dataset 3: SVM with RBF Kernel\n",
    "    4: RandomForestClassifier(n_estimators=100, random_state=42),  # Dataset 4: Random Forest\n",
    "    5: RandomForestClassifier(n_estimators=100, random_state=42),  # Dataset 5: Random Forest\n",
    "}\n",
    "\n",
    "# Loop through each dataset and train the respective model\n",
    "for dataset_index, model in model_mapping.items():\n",
    "    print(f\"Training model for Dataset {dataset_index}...\")\n",
    "    \n",
    "    # Load the corresponding training data and labels\n",
    "    train_data = train_datasets[f\"train_data_{dataset_index}\"]\n",
    "    train_label = train_labels[f\"train_label_{dataset_index}\"].values.flatten()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_data, train_label)\n",
    "    \n",
    "    # Save the trained model to a file\n",
    "    joblib.dump(model, f\"../models/model_{dataset_index}.pkl\")\n",
    "    print(f\"Model for Dataset {dataset_index} saved.\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca1961-c8c8-45c3-b421-27a75c9d192f",
   "metadata": {},
   "source": [
    "## 4. Store Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a2de81b-604a-466b-af14-846338bd6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for Validation Data 1...\n",
      "Predictions for Validation Data 1 saved to ../results/predictions_val_1.txt\n",
      "----------------------------------------\n",
      "Making predictions for Validation Data 2...\n",
      "Predictions for Validation Data 2 saved to ../results/predictions_val_2.txt\n",
      "----------------------------------------\n",
      "Making predictions for Validation Data 3...\n",
      "Predictions for Validation Data 3 saved to ../results/predictions_val_3.txt\n",
      "----------------------------------------\n",
      "Making predictions for Validation Data 4...\n",
      "Predictions for Validation Data 4 saved to ../results/predictions_val_4.txt\n",
      "----------------------------------------\n",
      "Making predictions for Validation Data 5...\n",
      "Predictions for Validation Data 5 saved to ../results/predictions_val_5.txt\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# File paths\n",
    "model_dir = \"../models\"\n",
    "results_dir = \"../results\"\n",
    "\n",
    "# Loop through each dataset, load model, make predictions and save the results\n",
    "for i in range(1, 6):  # Loop through datasets 1 to 5\n",
    "    print(f\"Making predictions for Validation Data {i}...\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = joblib.load(os.path.join(model_dir, f\"model_{i}.pkl\"))\n",
    "    \n",
    "    # Get the corresponding validation data (not test data)\n",
    "    validation_data = validation_datasets[f\"val_data_{i}\"]\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(validation_data)\n",
    "    \n",
    "    # Save the predictions to a file\n",
    "    predictions_file = os.path.join(results_dir, f\"predictions_val_{i}.txt\")\n",
    "    np.savetxt(predictions_file, predictions, fmt='%d', delimiter=\"\\n\")\n",
    "    \n",
    "    print(f\"Predictions for Validation Data {i} saved to {predictions_file}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059ab65-1b98-4020-b455-d420026bf0ff",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90a13290-cc7d-4185-88aa-d01c8f2ff6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating predictions for Validation Data 1...\n",
      "Dataset 1 Evaluation Metrics:\n",
      "  Accuracy: 0.9000\n",
      "  Precision: 0.9120\n",
      "  Recall: 0.9000\n",
      "  F1 Score: 0.8799\n",
      "----------------------------------------\n",
      "Evaluating predictions for Validation Data 2...\n",
      "Dataset 2 Evaluation Metrics:\n",
      "  Accuracy: 0.9500\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.9500\n",
      "  F1 Score: 0.9667\n",
      "----------------------------------------\n",
      "Evaluating predictions for Validation Data 3...\n",
      "Dataset 3 Evaluation Metrics:\n",
      "  Accuracy: 0.3532\n",
      "  Precision: 0.3189\n",
      "  Recall: 0.3532\n",
      "  F1 Score: 0.3064\n",
      "----------------------------------------\n",
      "Evaluating predictions for Validation Data 4...\n",
      "Dataset 4 Evaluation Metrics:\n",
      "  Accuracy: 0.9451\n",
      "  Precision: 0.9469\n",
      "  Recall: 0.9451\n",
      "  F1 Score: 0.9454\n",
      "----------------------------------------\n",
      "Evaluating predictions for Validation Data 5...\n",
      "Dataset 5 Evaluation Metrics:\n",
      "  Accuracy: 0.6696\n",
      "  Precision: 0.6703\n",
      "  Recall: 0.6696\n",
      "  F1 Score: 0.6951\n",
      "----------------------------------------\n",
      "Evaluation metrics saved to evaluation_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# File paths\n",
    "results_dir = \"../results\"\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "evaluation_metrics = {}\n",
    "\n",
    "for i in range(1, 6):  # Loop through datasets 1 to 5\n",
    "    print(f\"Evaluating predictions for Validation Data {i}...\")\n",
    "\n",
    "    # Load the ground truth validation labels\n",
    "    true_labels = validation_labels[f\"val_label_{i}\"].values.flatten()  # Flatten to ensure it's a 1D array\n",
    "    \n",
    "    # Load saved predictions for the validation set\n",
    "    predictions_file = os.path.join(results_dir, f\"predictions_val_{i}.txt\")\n",
    "    predictions = np.loadtxt(predictions_file, dtype=int)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted', zero_division=1)\n",
    "    recall = recall_score(true_labels, predictions, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted', zero_division=1)\n",
    "\n",
    "    # Store metrics\n",
    "    evaluation_metrics[f\"Dataset {i}\"] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Dataset {i} Evaluation Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save the evaluation metrics\n",
    "evaluation_metrics_df = pd.DataFrame(evaluation_metrics).T  # Transpose for readability\n",
    "evaluation_metrics_df.to_csv(os.path.join(results_dir, \"evaluation_metrics.csv\"))\n",
    "print(\"Evaluation metrics saved to evaluation_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470804c7-60ed-48cf-9279-fbf42178528c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
